# -*- coding: utf-8 -*-
"""VAEvsDCGAN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gPrHYCGSZWhWemELzZg23UCay8D_cOJV
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import torchvision.utils as vutils
from skimage.metrics import structural_similarity as ssim
import numpy as np
import matplotlib.pyplot as plt

transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)

train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)

class ConvVAE(nn.Module):
    def __init__(self, latent_dim=128):
        super(ConvVAE, self).__init__()
        # Encoder
        self.encoder = nn.Sequential(
            nn.Conv2d(3, 32, 4, 2, 1),
            nn.ReLU(),
            nn.Conv2d(32, 64, 4, 2, 1),
            nn.ReLU(),
            nn.Conv2d(64, 128, 4, 2, 1),
            nn.ReLU()
        )
        self.fc_mu = nn.Linear(128 * 4 * 4, latent_dim)
        self.fc_logvar = nn.Linear(128 * 4 * 4, latent_dim)

        # Decoder
        self.fc_decode = nn.Linear(latent_dim, 128 * 4 * 4)
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(128, 64, 4, 2, 1),
            nn.ReLU(),
            nn.ConvTranspose2d(64, 32, 4, 2, 1),
            nn.ReLU(),
            nn.ConvTranspose2d(32, 3, 4, 2, 1),
            nn.Tanh()
        )

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def forward(self, x):
        h = self.encoder(x)
        h = h.view(h.size(0), -1)
        mu, logvar = self.fc_mu(h), self.fc_logvar(h)
        z = self.reparameterize(mu, logvar)
        h_dec = self.fc_decode(z).view(-1, 128, 4, 4)
        return self.decoder(h_dec), mu, logvar

class DCGANGenerator(nn.Module):
    def __init__(self, latent_dim=100):
        super(DCGANGenerator, self).__init__()
        self.model = nn.Sequential(
            nn.ConvTranspose2d(latent_dim, 256, 4, 1, 0),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),

            nn.ConvTranspose2d(256, 128, 4, 2, 1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),

            nn.ConvTranspose2d(128, 64, 4, 2, 1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),

            nn.ConvTranspose2d(64, 3, 4, 2, 1),
            nn.Tanh()
        )

    def forward(self, x):
        return self.model(x)

class DCGANDiscriminator(nn.Module):
    def __init__(self):
        super(DCGANDiscriminator, self).__init__()
        self.model = nn.Sequential(
            # Input: 3 x 32 x 32
            nn.Conv2d(3, 64, 4, 2, 1),
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(64, 128, 4, 2, 1),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(128, 256, 4, 2, 1),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(256, 1, 4, 1, 0),
            nn.Sigmoid()
        )

    def forward(self, x):
        # Return shape [batch_size, 1] without squeezing
        return self.model(x).view(-1, 1)

def train_dcgan_discriminator(discriminator, real_images, fake_images, d_optimizer):
    batch_size = real_images.size(0)
    d_optimizer.zero_grad()

    # Train with real images
    real_labels = torch.ones(batch_size, 1, device=real_images.device)
    real_output = discriminator(real_images)
    d_real_loss = nn.BCELoss()(real_output, real_labels)

    # Train with fake images
    fake_labels = torch.zeros(batch_size, 1, device=fake_images.device)
    fake_output = discriminator(fake_images.detach())
    d_fake_loss = nn.BCELoss()(fake_output, fake_labels)

    # Total discriminator loss
    d_loss = d_real_loss + d_fake_loss
    d_loss.backward()
    d_optimizer.step()

    return d_loss

def train_dcgan_generator(generator, discriminator, fake_images, g_optimizer):
    batch_size = fake_images.size(0)
    g_optimizer.zero_grad()

    # Labels for generated images (wants discriminator to believe they are real)
    labels = torch.ones(batch_size, 1, device=fake_images.device)

    # Calculate generator loss
    output = discriminator(fake_images)
    g_loss = nn.BCELoss()(output, labels)
    g_loss.backward()
    g_optimizer.step()

    return g_loss

import torch.nn.functional as F

def vae_loss(recon_images, images, mu, logvar):
    # Reconstruction Loss
    recon_loss = F.mse_loss(recon_images, images, reduction='sum')

    # KL Divergence
    kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())

    # Total VAE Loss
    return recon_loss + kl_divergence

import time

# Lists to store metrics for VAE
vae_train_loss = []
vae_epochs = []
vae_time_per_epoch = []

# Modified VAE Training Function
def train_vae(model, dataloader, optimizer, num_epochs=10):
    model.train()
    for epoch in range(num_epochs):
        start_time = time.time()
        total_loss = 0

        for images, _ in dataloader:
            images = images.to(device)
            optimizer.zero_grad()
            recon_images, mu, logvar = model(images)
            loss = vae_loss(recon_images, images, mu, logvar)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        avg_loss = total_loss / len(dataloader.dataset)
        vae_train_loss.append(avg_loss)
        vae_epochs.append(epoch + 1)
        vae_time_per_epoch.append(time.time() - start_time)

        print(f'Epoch [{epoch+1}/{num_epochs}], VAE Loss: {avg_loss:.4f}, Time per Epoch: {vae_time_per_epoch[-1]:.2f}s')

import torch.optim as optim

# Define the optimizer for the VAE model
vae_optimizer = optim.Adam(vae_model.parameters(), lr=0.001)
generator = DCGANGenerator(latent_dim=100).to(device)
discriminator = DCGANDiscriminator().to(device)

# Initialize optimizers
g_optimizer = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))
d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))

# Training VAE
train_vae(vae_model, train_loader, vae_optimizer, num_epochs=100)

import matplotlib.pyplot as plt

# Plot VAE Training Loss and Time Complexity
plt.figure(figsize=(12, 6))

# Plot VAE Loss
plt.subplot(1, 2, 1)
plt.plot(vae_epochs, vae_train_loss, label="VAE Training Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("VAE Training Loss")

# Plot VAE Time per Epoch
plt.subplot(1, 2, 2)
plt.plot(vae_epochs, vae_time_per_epoch, label="VAE Time per Epoch", color="orange")
plt.xlabel("Epochs")
plt.ylabel("Time (seconds)")
plt.title("VAE Time Complexity per Epoch")

plt.show()

import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import time

def train_dcgan(generator, discriminator, dataloader, g_optimizer, d_optimizer, device, num_epochs=10, latent_dim=100):
    # Initialize the metrics lists
    dcgan_g_loss = []
    dcgan_d_loss = []
    dcgan_epochs = []
    dcgan_time_per_epoch = []

    for epoch in range(num_epochs):
        start_time = time.time()
        g_loss_epoch = 0
        d_loss_epoch = 0

        for real_images, _ in dataloader:
            batch_size = real_images.size(0)
            real_images = real_images.to(device)

            # Generate fake images
            noise = torch.randn(batch_size, latent_dim, 1, 1).to(device)
            fake_images = generator(noise)

            # Update Discriminator
            d_loss = train_dcgan_discriminator(discriminator, real_images, fake_images, d_optimizer)
            d_loss_epoch += d_loss.item()

            # Update Generator
            g_loss = train_dcgan_generator(generator, discriminator, fake_images, g_optimizer)
            g_loss_epoch += g_loss.item()

        # Calculate average losses for the epoch
        g_loss_epoch /= len(dataloader)
        d_loss_epoch /= len(dataloader)

        # Store metrics
        dcgan_g_loss.append(g_loss_epoch)
        dcgan_d_loss.append(d_loss_epoch)
        dcgan_epochs.append(epoch + 1)
        dcgan_time_per_epoch.append(time.time() - start_time)

        print(f'Epoch [{epoch+1}/{num_epochs}], '
              f'Generator Loss: {g_loss_epoch:.4f}, '
              f'Discriminator Loss: {d_loss_epoch:.4f}, '
              f'Time per Epoch: {dcgan_time_per_epoch[-1]:.2f}s')

    return dcgan_g_loss, dcgan_d_loss, dcgan_epochs, dcgan_time_per_epoch

def plot_dcgan_metrics(dcgan_epochs, dcgan_g_loss, dcgan_d_loss, dcgan_time_per_epoch):
    """
    Plot DCGAN training metrics.
    """
    plt.figure(figsize=(12, 6))

    # Plot DCGAN Loss
    plt.subplot(1, 2, 1)
    plt.plot(dcgan_epochs, dcgan_g_loss, label="Generator Loss", color="blue", marker='o')
    plt.plot(dcgan_epochs, dcgan_d_loss, label="Discriminator Loss", color="red", marker='o')
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.title("DCGAN Generator and Discriminator Losses")
    plt.legend()
    plt.grid(True)

    # Plot DCGAN Time per Epoch
    plt.subplot(1, 2, 2)
    plt.plot(dcgan_epochs, dcgan_time_per_epoch, label="DCGAN Time per Epoch",
             color="purple", marker='o')
    plt.xlabel("Epochs")
    plt.ylabel("Time (seconds)")
    plt.title("DCGAN Time Complexity per Epoch")
    plt.grid(True)

    plt.tight_layout()
    plt.show()

# Usage example:
# Train the model
g_losses, d_losses, epochs, times = train_dcgan(
    generator,
    discriminator,
    train_loader,
    g_optimizer,
    d_optimizer,
    device,
    num_epochs=100
)

# Plot the results
plot_dcgan_metrics(epochs, g_losses, d_losses, times)

from skimage.metrics import structural_similarity as ssim

def calculate_mse_ssim(model, dataloader):
    model.eval()
    mse_loss = nn.MSELoss()
    total_mse, total_ssim = 0, 0
    with torch.no_grad():
        for images, _ in dataloader:
            images = images.to(device)
            if isinstance(model, ConvVAE):
                recon_images, _, _ = model(images)
            else:
                noise = torch.randn(images.size(0), 100, 1, 1).to(device)
                recon_images = model(noise)
            mse = mse_loss(recon_images, images).item()
            total_mse += mse
            # Calculate SSIM with specified data_range
            ssim_score = np.mean([
                ssim(img1, img2, win_size=3, data_range=1.0, channel_axis=-1)
                for img1, img2 in zip(images.cpu().numpy(), recon_images.cpu().numpy())
            ])
            total_ssim += ssim_score
    return total_mse / len(dataloader), total_ssim / len(dataloader)

# Run the calculation
torch_mse, torch_ssim = calculate_mse_ssim(vae_model, test_loader)
print(f'MSE: {torch_mse:.4f}, SSIM: {torch_ssim:.4f}')

def calculate_mse_ssim(model, data_loader, device, is_dcgan=False, noise_dim=None):
    """
    Calculate MSE and SSIM metrics for model reconstructions.

    Args:
        model: The model (VAE or DCGAN) to evaluate
        data_loader: DataLoader containing the test images
        device: torch device (cuda/cpu)
        is_dcgan: Boolean indicating if the model is DCGAN
        noise_dim: Specified dimensionality for the DCGAN noise vector

    Returns:
        tuple: (average_mse, average_ssim)
    """
    model.eval()
    total_mse = 0
    total_ssim = 0
    num_images = 0
    num_ssim_calcs = 0  # Separate counter for SSIM calculations

    with torch.no_grad():
        for batch in tqdm(data_loader, desc="Calculating metrics"):
            inputs, _ = batch
            inputs = inputs.to(device)

            try:
                if is_dcgan:
                    # Check if noise dimension is provided
                    if noise_dim is None:
                        raise ValueError("Noise dimension (noise_dim) must be provided for DCGAN.")

                    # Generate noise and use the DCGAN generator
                    noise = torch.randn(inputs.size(0), noise_dim, 1, 1, device=device)
                    reconstructed = model(noise)  # using the generator to generate images
                elif isinstance(model, ConvVAE):
                    reconstructed, _, _ = model(inputs)
                else:
                    raise ValueError(f"Unsupported model type: {type(model)}")

                # Ensure outputs are in valid range [0, 1]
                reconstructed = torch.clamp(reconstructed, 0, 1)

                # Move to CPU and convert to numpy
                inputs = inputs.cpu().numpy()
                reconstructed = reconstructed.cpu().numpy()

                for i in range(inputs.shape[0]):
                    original_image = np.transpose(inputs[i], (1, 2, 0))
                    reconstructed_image = np.transpose(reconstructed[i], (1, 2, 0))

                    mse_value = np.mean((original_image - reconstructed_image) ** 2)
                    total_mse += mse_value

                    if original_image.shape[0] >= 7 and original_image.shape[1] >= 7:
                        ssim_value = ssim(
                            original_image,
                            reconstructed_image,
                            data_range=1.0,
                            channel_axis=-1,
                            win_size=min(7, min(original_image.shape[0], original_image.shape[1]) // 2 * 2 + 1)
                        )
                        total_ssim += ssim_value
                        num_ssim_calcs += 1

                    num_images += 1

            except Exception as e:
                print(f"Error processing batch: {str(e)}")
                continue

    avg_mse = total_mse / num_images if num_images > 0 else float('nan')
    avg_ssim = total_ssim / num_ssim_calcs if num_ssim_calcs > 0 else float('nan')

    print(f"Processed {num_images} images")
    print(f"SSIM calculated for {num_ssim_calcs} images")

    return avg_mse, avg_ssim

# Usage
try:
    # Calculate metrics for VAE
    print("\nCalculating VAE metrics...")
    vae_mse, vae_ssim = calculate_mse_ssim(vae_model, test_loader, device)

    # Set noise_dim according to your DCGAN's architecture
    noise_dim = 100  # Example, set this to your actual noise dimension

    # Calculate metrics for DCGAN
    print("\nCalculating DCGAN metrics...")
    dcgan_mse, dcgan_ssim = calculate_mse_ssim(generator, test_loader, device, is_dcgan=True, noise_dim=noise_dim)

    # Print results
    print("\nResults:")
    print(f"VAE   - MSE: {vae_mse:.4f}, SSIM: {vae_ssim:.4f}")
    print(f"DCGAN - MSE: {dcgan_mse:.4f}, SSIM: {dcgan_ssim:.4f}")

except Exception as e:
    print(f"An error occurred during metric calculation: {str(e)}")